{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6072db-a74c-478e-b547-7549c1e63ba2",
   "metadata": {},
   "source": [
    "Min-max scaling, also known as min-max normalization, is a data preprocessing technique used to rescale numerical features to a specific range, typically between 0 and 1. This transformation preserves the relative relationships between the data points while ensuring that all features have the same scale. It is particularly useful when the features have different ranges or units, and it helps improve the performance of machine learning algorithms, especially those sensitive to feature scales, such as support vector machines (SVM) or k-nearest neighbors (KNN).\n",
    "\n",
    "Here's how min-max scaling works:\n",
    "\n",
    "1. Find the Minimum and Maximum Values: For each feature, determine the minimum (min) and maximum (max) values across the entire dataset.\n",
    "\n",
    "2. Scale the Values: For each feature \\( x \\) in the dataset, apply the following formula to scale the values to the range [0, 1]:\n",
    "   \n",
    "   \\[ x_{\\text{scaled}} = \\frac{x - \\text{min}}{\\text{max} - \\text{min}} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\( x_{\\text{scaled}} \\) is the scaled value of feature \\( x \\),\n",
    "   - \\( x \\) is the original value of the feature,\n",
    "   - \\( \\text{min} \\) is the minimum value of the feature across the dataset,\n",
    "   - \\( \\text{max} \\) is the maximum value of the feature across the dataset.\n",
    "\n",
    "3. Repeat for Each Feature: Apply the scaling process to each numerical feature in the dataset.\n",
    "\n",
    "Here's an example to illustrate the application of min-max scaling:\n",
    "\n",
    "Consider a dataset containing two numerical features: house area (in square feet) and number of bedrooms. The original values for these features are as follows:\n",
    "\n",
    "- House area: [1500, 2000, 1800, 2200, 1600]\n",
    "- Number of bedrooms: [3, 4, 3, 5, 2]\n",
    "\n",
    "To apply min-max scaling:\n",
    "\n",
    "1. Find the minimum and maximum values for each feature:\n",
    "   - House area: min = 1500, max = 2200\n",
    "   - Number of bedrooms: min = 2, max = 5\n",
    "\n",
    "2. Scale the values for each feature using the min-max scaling formula:\n",
    "   - Scaled house area: \\([0.0, 0.666, 0.333, 1.0, 0.166]\\)\n",
    "   - Scaled number of bedrooms: \\([0.5, 1.0, 0.5, 1.0, 0.0]\\)\n",
    "\n",
    "The scaled values now fall within the range [0, 1], preserving the relative relationships between the data points while ensuring that all features have the same scale. This scaled dataset can now be used as input for machine learning algorithms, facilitating better convergence and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9800d6d-e2d6-48bc-8ec7-98d0cc15484f",
   "metadata": {},
   "source": [
    "The unit vector technique, also known as unit normalization or vector normalization, is a feature scaling method used to scale numerical features to have a unit norm, typically a length of 1. Unlike min-max scaling, which scales features to a specific range, unit vector scaling focuses on normalizing the features' magnitudes while preserving their directions. This technique is commonly used in scenarios where the direction of the feature vectors is important, such as in machine learning algorithms that rely on distance metrics like cosine similarity or Euclidean distance.\n",
    "\n",
    "Here's how the unit vector technique works:\n",
    "\n",
    "1. Compute the Norm: For each feature vector, calculate its Euclidean norm, which represents its magnitude or length:\n",
    "   \n",
    "   \\[ \\text{Norm} = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\( x_1, x_2, \\ldots, x_n \\) are the components of the feature vector.\n",
    "\n",
    "2. Scale the Vector: Divide each component of the feature vector by its norm to scale the vector to have a unit length:\n",
    "   \n",
    "   \\[ x_{\\text{unit}} = \\frac{x}{\\text{Norm}} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\( x_{\\text{unit}} \\) is the scaled feature vector with a unit norm,\n",
    "   - \\( x \\) is the original feature vector,\n",
    "   - \\( \\text{Norm} \\) is the Euclidean norm of the feature vector.\n",
    "\n",
    "3. Repeat for Each Feature: Apply the unit vector scaling process to each feature vector in the dataset.\n",
    "\n",
    "Here's an example to illustrate the application of the unit vector technique:\n",
    "\n",
    "Consider a dataset containing two numerical features: house area (in square feet) and number of bedrooms. The original feature vectors for these features are as follows:\n",
    "\n",
    "- House area: [1500, 2000, 1800, 2200, 1600]\n",
    "- Number of bedrooms: [3, 4, 3, 5, 2]\n",
    "\n",
    "To apply the unit vector technique:\n",
    "\n",
    "1. Compute the norm for each feature vector:\n",
    "   - House area: \\( \\text{Norm} = \\sqrt{1500^2 + 2000^2 + 1800^2 + 2200^2 + 1600^2} \\)\n",
    "   - Number of bedrooms: \\( \\text{Norm} = \\sqrt{3^2 + 4^2 + 3^2 + 5^2 + 2^2} \\)\n",
    "\n",
    "2. Scale each feature vector to have a unit norm by dividing each component by its norm:\n",
    "   - Scaled house area: \\([0.231, 0.309, 0.278, 0.341, 0.247]\\)\n",
    "   - Scaled number of bedrooms: \\([0.371, 0.495, 0.371, 0.619, 0.247]\\)\n",
    "\n",
    "The scaled feature vectors now have a unit norm, ensuring that their magnitudes are consistent while preserving their directions. This technique is particularly useful in scenarios where the relative importance of feature magnitudes is more significant than their absolute values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e12d74-b5e0-4fcc-a243-661fb7166e25",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most important information. PCA identifies the principal components (PCs) of the data, which are orthogonal vectors that capture the maximum variance in the original dataset. These principal components represent the directions of maximum variability in the data and can be used to project the data onto a lower-dimensional subspace.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1. Compute the Covariance Matrix: Calculate the covariance matrix of the original dataset, which represents the relationships between different features. This matrix quantifies how each feature varies with respect to others.\n",
    "\n",
    "2. Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix to obtain its eigenvectors and eigenvalues. The eigenvectors represent the principal components, while the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "3. Select Principal Components: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The principal components with the highest eigenvalues capture the most variance in the data and are selected for dimensionality reduction.\n",
    "\n",
    "4. Project Data onto Principal Components: Project the original data onto the selected principal components to obtain a lower-dimensional representation of the dataset. This transformation reduces the dimensionality of the data while preserving as much variance as possible.\n",
    "\n",
    "PCA is commonly used in dimensionality reduction to reduce the complexity of high-dimensional datasets and facilitate data visualization, exploration, and analysis. By reducing the number of features, PCA can also help improve the performance of machine learning algorithms by reducing the risk of overfitting and computational complexity.\n",
    "\n",
    "Here's an example to illustrate the application of PCA for dimensionality reduction:\n",
    "\n",
    "Consider a dataset containing information about houses, including features such as size (in square feet), number of bedrooms, number of bathrooms, and location (latitude and longitude). This dataset has five-dimensional data (five features).\n",
    "\n",
    "To apply PCA:\n",
    "\n",
    "1. Compute the covariance matrix of the original dataset.\n",
    "\n",
    "2. Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "3. Select the principal components based on their corresponding eigenvalues. Let's say we choose the first three principal components, as they capture the most variance in the data.\n",
    "\n",
    "4. Project the original data onto the selected principal components to obtain a lower-dimensional representation of the dataset. Now, instead of five features, we have three principal components that capture the most important information in the dataset.\n",
    "\n",
    "The reduced-dimensional dataset can be used for further analysis, visualization, or model building, providing a more manageable representation of the original high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07661174-9ec0-4a0d-bf77-2b1318d67312",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts, as PCA can be used as a technique for feature extraction. Feature extraction refers to the process of transforming raw data into a new set of features that captures the most important information while reducing dimensionality. PCA accomplishes this by identifying the principal components (PCs) of the data, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "1. Compute Principal Components: PCA identifies the principal components of the data by performing eigenvalue decomposition on the covariance matrix of the original dataset. Each principal component is a linear combination of the original features, with weights determined by the eigenvectors of the covariance matrix.\n",
    "\n",
    "2. Select Principal Components: The principal components are sorted based on their corresponding eigenvalues, with the first few components capturing the most variance in the data. These principal components serve as the new set of features for the transformed dataset.\n",
    "\n",
    "3. Project Data onto Principal Components: The original data is projected onto the selected principal components to obtain a lower-dimensional representation of the dataset. This transformation reduces the dimensionality of the data while preserving as much variance as possible.\n",
    "\n",
    "By using PCA for feature extraction, the dimensionality of the dataset is reduced, and the new set of features captures the most important information in the data. This can lead to improved model performance, reduced computational complexity, and enhanced interpretability.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Consider a dataset containing grayscale images of handwritten digits, each represented as a matrix of pixel values. Each image has a high dimensionality, with each pixel serving as a feature. To extract features using PCA:\n",
    "\n",
    "1. Compute the covariance matrix of the pixel values across all images.\n",
    "\n",
    "2. Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues.\n",
    "\n",
    "3. Select the principal components based on their corresponding eigenvalues. Let's say we choose the first 50 principal components, as they capture the most variance in the image data.\n",
    "\n",
    "4. Project the original images onto the selected principal components to obtain a lower-dimensional representation of the dataset. Each image is now represented by a vector of 50 principal component scores, which serve as the new set of features.\n",
    "\n",
    "The reduced-dimensional feature representation obtained from PCA can be used as input for machine learning algorithms for tasks such as image classification, where the original high-dimensional pixel features are transformed into a more compact and informative representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5574f-2e63-4ee2-802d-424c971a971f",
   "metadata": {},
   "source": [
    "To perform min-max scaling to transform the values to a range of -1 to 1, follow these steps:\n",
    "\n",
    "1. Calculate the minimum and maximum values of the dataset.\n",
    "2. Apply the min-max scaling formula to scale each value to the desired range.\n",
    "\n",
    "Let's perform these steps:\n",
    "\n",
    "1. Calculate the minimum and maximum values:\n",
    "   - Minimum value: 1\n",
    "   - Maximum value: 20\n",
    "\n",
    "2. Apply the min-max scaling formula:\n",
    "   \\[ x_{\\text{scaled}} = \\frac{x - \\text{min}}{\\text{max} - \\text{min}} \\times (max\\_range - min\\_range) + min\\_range \\]\n",
    "\n",
    "   Here, \\( x \\) represents each value in the dataset, \\( \\text{min} \\) and \\( \\text{max} \\) represent the minimum and maximum values of the dataset, and \\( min\\_range \\) and \\( max\\_range \\) represent the desired range (-1 to 1).\n",
    "\n",
    "   \\[ x_{\\text{scaled}} = \\frac{x - 1}{20 - 1} \\times (1 - (-1)) + (-1) \\]\n",
    "\n",
    "Let's calculate the scaled values:\n",
    "\n",
    "- For \\( x = 1 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{1 - 1}{20 - 1} \\times (1 - (-1)) + (-1) = -1 \\]\n",
    "\n",
    "- For \\( x = 5 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{5 - 1}{20 - 1} \\times (1 - (-1)) + (-1) = -0.6 \\]\n",
    "\n",
    "- For \\( x = 10 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{10 - 1}{20 - 1} \\times (1 - (-1)) + (-1) = 0.2 \\]\n",
    "\n",
    "- For \\( x = 15 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{15 - 1}{20 - 1} \\times (1 - (-1)) + (-1) = 0.6 \\]\n",
    "\n",
    "- For \\( x = 20 \\):\n",
    "   \\[ x_{\\text{scaled}} = \\frac{20 - 1}{20 - 1} \\times (1 - (-1)) + (-1) = 1 \\]\n",
    "\n",
    "So, the scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are [-1, -0.6, 0.2, 0.6, 1], respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
